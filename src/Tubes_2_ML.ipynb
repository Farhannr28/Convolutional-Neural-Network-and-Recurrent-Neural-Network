{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV5cnYjbYqcj"
      },
      "source": [
        "# Tubes 2 ML - CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ5omBZVZAeK"
      },
      "source": [
        "## Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0vhdrPgXXH5",
        "outputId": "35bd08f8-18ca-45df-997c-2eda2809dbc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import tensorflow as tf\n",
        "    print(\"TensorFlow version:\", tf.__version__)\n",
        "except ImportError:\n",
        "    %pip install tensorflow\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras import layers, models, losses, optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm1ujbePY80T"
      },
      "source": [
        "## Import CNN Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8dFolcXYxjL",
        "outputId": "c0ddc1d0-7b05-4b0a-b841-02ba9d6cfb47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train shape: (50000, 32, 32, 3) (50000, 1)\n",
            "Test shape: (10000, 32, 32, 3) (10000, 1)\n",
            "Train/Val/Test sizes: 40000 10000 10000\n"
          ]
        }
      ],
      "source": [
        "# Load CIFAR-10 using tf.keras.datasets\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "print(\"Train shape:\", x_train.shape, y_train.shape)\n",
        "print(\"Test shape:\", x_test.shape, y_test.shape)\n",
        "\n",
        "# Normalize to [0, 1]\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "y_train, y_test = y_train.flatten(), y_test.flatten()\n",
        "\n",
        "# Split train to 40k train / 10k val\n",
        "x_val, y_val = x_train[40000:], y_train[40000:]\n",
        "x_train, y_train = x_train[:40000], y_train[:40000]\n",
        "\n",
        "print(\"Train/Val/Test sizes:\", len(x_train), len(x_val), len(x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPZnH5izaacY"
      },
      "source": [
        "## Create CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rWbtE7FHY4Kq"
      },
      "outputs": [],
      "source": [
        "class CNNModel:\n",
        "    def __init__(self, conv_layers=2, filters=32, kernel_size=3, pooling='max'):\n",
        "        self.conv_layers = conv_layers\n",
        "        self.filters = filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.pooling = pooling\n",
        "        self.model = self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = models.Sequential()\n",
        "        model.add(layers.Input(shape=(32, 32, 3)))\n",
        "\n",
        "        for _ in range(self.conv_layers):\n",
        "            model.add(layers.Conv2D(self.filters, self.kernel_size, activation='relu', padding='same'))\n",
        "            if self.pooling == 'max':\n",
        "                model.add(layers.MaxPooling2D())\n",
        "            else:\n",
        "                model.add(layers.AveragePooling2D())\n",
        "\n",
        "        model.add(layers.Flatten())\n",
        "        model.add(layers.Dense(128, activation='relu'))\n",
        "        model.add(layers.Dense(10))\n",
        "        return model\n",
        "\n",
        "    def compile(self):\n",
        "        self.model.compile(\n",
        "            loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "            optimizer=optimizers.Adam(),\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "    def train(self, x_train, y_train, x_val, y_val, epochs=2, batch_size=64):\n",
        "        return self.model.fit(\n",
        "            x_train, y_train,\n",
        "            validation_data=(x_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "\n",
        "    def evaluate(self, x_test, y_test):\n",
        "        y_pred_logits = self.model.predict(x_test)\n",
        "        y_pred = np.argmax(y_pred_logits, axis=1)\n",
        "        f1 = f1_score(y_test, y_pred, average='macro')\n",
        "        print(\"Test Macro F1 Score:\", f1)\n",
        "        return f1\n",
        "\n",
        "    def save_weights(self, path=\"cnn.weights.h5\"):\n",
        "        self.model.save_weights(path)\n",
        "\n",
        "    def load_weights(self, path=\"cnn.weights.h5\"):\n",
        "        self.model.load_weights(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88v8vK6UahQa"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5BEVydzarx4",
        "outputId": "dfdb5501-2e4e-42a4-d76c-7572725c6804"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 89ms/step - accuracy: 0.3581 - loss: 1.7643 - val_accuracy: 0.5697 - val_loss: 1.2360\n",
            "Epoch 2/2\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 88ms/step - accuracy: 0.5803 - loss: 1.1907 - val_accuracy: 0.6150 - val_loss: 1.0984\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step\n",
            "Test Macro F1 Score: 0.604847141807627\n"
          ]
        }
      ],
      "source": [
        "class TrainCNN:\n",
        "    def __init__(self, x_train, y_train, x_val, y_val, x_test, y_test):\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.x_val = x_val\n",
        "        self.y_val = y_val\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "\n",
        "    def run(self):\n",
        "        model = CNNModel()\n",
        "        model.compile()\n",
        "        model.train(x_train, y_train, x_val, y_val)\n",
        "        model.save_weights()\n",
        "        model.evaluate(x_test, y_test)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  trainer = TrainCNN(x_train, y_train, x_val, y_val, x_test, y_test)\n",
        "  trainer.run()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
